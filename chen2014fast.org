#+TITLE:A Fast and Accurate Dependency Parser using Neural Networks.

* Summary
  
This paper applies dense word embeddings and neural networks to the
problem of syntactic parsing. Syntactic parsing is the process of
taking a sentence and constructing a meaningful data structure from
it. It is similar to parsing a programming language except that in
natural language there is a great deal of ambiguity, namely given an
input sentence there are many possible data structures that reflect
valid syntax. However, if you ask any human, most of them will be
nonsense. The authors consider a specific type of transition-based
parser called the arc-standard. A transition-based parser iterates
over a sentence one word at a time and updates its internal
representation of the parsed sentence (a tree or a graph) as it
goes. The arc-standard defines a few data structures, namely a stack,
a buffer and a set of arc (graph edges), and a number of transitions
that update those data structures at each step. This paper uses
embeddings and neural networks to approximate the oracle that chooses
the best transition at each step in the process. The result is a
parser that performs slightly better than the state of the art in
terms of accuracy, but is orders of magnitude faster.

* High Level Paper
  
To anyone in the field of NLP and with some knowledge of syntactic
parsing, this paper is probably quite clear. The authors take steps to
make their intent and contributions clear. However, they do a poor job
of putting this work in the larger context of machine learning and
making it relevant to those who might not be interested in syntactic
parsing or NLP. Considering that the algorithm is built out of two
parts: one deterministic and based on ubiquitous data structures and
ideas in CS, and one piece of innovation taking advantage of the
recent advances in embeddings and neural networks, there are certainly
larger themes and ideas that could be elaborated upon. Nonetheless, it
is refreshing to read a paper that sticks to the facts and does not
attempt to go out-of-scope. Unfortunately, this means that the
audience will be smaller.

As the authors themselves are first to point out, this is highly
original work that makes a significant advance for syntactic parsing
when it comes to speed. It is quite exciting to see the success of
dense embeddings applied more broadly in NLP. Furthermore, the
problems that embeddings improve on --- namely, the problem of sparse
data --- may be relevant in other fields as well.

The experiments clearly demonstrate the great speed improvement. This
should be the main story of the paper, not the improvement in
accuracy. Otherwise, the experiments seem to sufficiently test the
claims made in the paper.

* High Level Technical
  
The cubic activation function is interesting, but to be fair one
should include RELU as well. Figure 4 would be the place to include
this. I'm very interested to know why RELU was not tried. The authors
experiment with different size hidden layer, but restrict themselves
to a single hidden layer. Why not try multiple hidden layers? This is
another interesting experiment.

* Low Level Technical
  
It is somewhat misleading to emphasize the superior accuracy of the
parser, especially when there is no standard deviation provided for
the accuracy measurements. There is variance in the pretrained
embeddings as well as in the parameter initialization but this is not
reported. Overall, this emphasis takes away from the real contribution
which is a fast parser with an interpret-able embedding layer.

Figure 6 is quite difficult to read. What is the meaning of the gray
background?

* Review Summary
  
The authors demonstrate the effectiveness of using neural networks and
embeddings to approximate the oracle in a transition-based parser,
specifically in the arc-standard. It's a fascinating application of
neural embeddings combined with standard stack-based algorithms to
provide a syntactic parser that blows away the state of the art when
it comes to parsing speed. It's also fascinating to see how the neural
embedding is able to find the relevant features, both high- and
low-order, to the task of syntactic parsing.

Not coming from an NLP background myself, I would be interested in the
broader context of this work.

* Next Step
  
The authors themselves suggest doing beam search instead of simply
selecting the single best action at every step of the
algorithm. Generally, the idea of "joint-embeddings" across related
but diverse spaces is very interesting, specifically combining word
embeddings, POS embeddings and arc embeddings into a single model.

#  LocalWords:  embeddings NLP
